{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the functions have one or more places where you are to \"FILL THIS IN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW7\n",
    "# Hannah Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBM:\n",
    "\n",
    "    def __init__(self,S0,r,sigma):\n",
    "        self.S0 = 1\n",
    "        self.r = 0.03\n",
    "        self.sigma = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw7dynamics = GBM(S0=1,r=0.03,sigma=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Put:\n",
    "    \n",
    "    def __init__(self,K,T):\n",
    "        self.K = K\n",
    "        self.T = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw7contract = Put(K=1.1,T=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC:\n",
    "    \n",
    "    def __init__(self,M,N,seed,algorithm):\n",
    "        \n",
    "        self.M = M # Number of paths  \n",
    "        self.N = N     # Number of time periods  \n",
    "        self.rng = np.random.default_rng(seed=seed) # Seeding the random number generator with a specified number helps make the calculations reproducible\n",
    "\n",
    "        self.algorithm = algorithm   \n",
    "        #'value' for Value-based approach (Longstaff-Schwartz) -- problem 1a\n",
    "        #'policy' for Policy optimization -- problem 1b\n",
    "    \n",
    "    def price_americanPut_GBM(self,contract,dynamics):\n",
    "\n",
    "        r=dynamics.r\n",
    "        sigma=dynamics.sigma\n",
    "        S0=dynamics.S0\n",
    "\n",
    "        K=contract.K\n",
    "        T=contract.T\n",
    "\n",
    "        N=self.N\n",
    "        M=self.M\n",
    "        dt=T/N\n",
    "\n",
    "        Z = self.rng.normal(size=(M,N))\n",
    "\n",
    "        paths = S0*np.exp((r-sigma**2/2)*dt*np.tile(np.arange(1,N+1),(M,1))+sigma*np.sqrt(dt)*np.cumsum(Z,axis=1))\n",
    "\n",
    "        payoffDiscounted = np.maximum(0,K-paths[:,-1])\n",
    "        #This is the payoff (cashflow) along each path,\n",
    "        #discounted to time nn (for nn=N,N-1,...)\n",
    "        #It corresponds to the far right-hand column in each page of the\n",
    "        #Excel worksheet\n",
    "        #I'm initializing it for time nn=N.\n",
    "\n",
    "        #You could make payoffDiscounted\n",
    "        #to be a matrix because it depends on nn.\n",
    "        #But I will just reuse a 1-dimensional array,\n",
    "        #by overwriting the time nn+1 entries at time nn.        \n",
    "\n",
    "        for nn in np.arange(N-1,0,-1):\n",
    "            continuationPayoffDiscounted = np.exp(-r*dt)*payoffDiscounted\n",
    "            # This is the CONTINUATION payoff (cashflow) along each path,\n",
    "            # discounted to time nn (for nn=N-1,N-2,...)\n",
    "            # It corresponds to the blue column in each page of the Excel worksheet\n",
    "            # Note that payoffDiscounted comes from the previous iteration \n",
    "            # -- which was at time nn+1.  So now we discount back to time nn.\n",
    "\n",
    "            X=paths[:,nn-1]               \n",
    "            exerciseValue = K-X\n",
    "\n",
    "            if self.algorithm == 'value': \n",
    "                # This is the value function (Longstaff-Schwartz) approach.  For problem 1a\n",
    "\n",
    "                basisfunctions = np.stack([np.ones(M), X, X**2], axis=1)\n",
    "                # FILL THIS IN.  You may use np.stack\n",
    "                        # This will be an M-by-3 array containing the basis functions (Same ones as L7.9-7.10, and Excel)\n",
    "\n",
    "                coefficients = sm.OLS(continuationPayoffDiscounted, basisfunctions).fit().params\n",
    "                # FILL THIS IN  \n",
    "                        # This will be an array of 3 estimated \"betas\".\n",
    "\n",
    "                estimatedContinuationValue = basisfunctions  @ coefficients\n",
    "                # FILL THIS IN with an array of length M. \n",
    "                        # This is similar to the Red column in Excel\n",
    "\n",
    "                whichPathsToExercise = (exerciseValue >= np.maximum(estimatedContinuationValue,0))\n",
    "                        #This is a length-M array of Booleans\n",
    "\n",
    "            elif self.algorithm == 'policy':\n",
    "                # This is the policy optimization approach to Reinforcement learning.  For problem 1b\n",
    "\n",
    "                (a_opt,b_opt) = scipy.optimize.minimize(\n",
    "                    negofMCaverageOfExpectedPayouts,(0,0),args=(X,exerciseValue,continuationPayoffDiscounted),method='Nelder-Mead').x\n",
    "                    #Chose Nelder-Mead optimizer because it is generating reasonable results with minimal coding effort\n",
    "                    #But gradient methods, done properly, usually run faster\n",
    "\n",
    "                whichPathsToExercise = softExercise(X,a_opt,b_opt)>0.5\n",
    "                    #FILL THIS IN, using the right-hand side of the last equation on the homework sheet \n",
    "                    #This obtains the hard exercise decision from the optimized soft exercise function\n",
    "                    #It should be a length-M array of Booleans (as it was in the \"value\" approach.  \n",
    "                    #But here it comes from the softExercise function)\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Unknown algorithm type')\n",
    "\n",
    "\n",
    "            payoffDiscounted[whichPathsToExercise] = exerciseValue[whichPathsToExercise]\n",
    "            # FILL THIS IN -- see the \"discounted cashflow along path\" column in Excel \n",
    "            payoffDiscounted[np.logical_not(whichPathsToExercise)] = 0 \n",
    "            # FILL THIS IN -- see the \"discounted cashflow along path\" column in Excel\n",
    "\n",
    "        # The time-0 calculation needs no regression\n",
    "        continuationPayoffDiscounted = np.exp(-r*dt)*payoffDiscounted;\n",
    "        estimatedContinuationValue = np.mean(continuationPayoffDiscounted);\n",
    "        putprice = max(K-S0,estimatedContinuationValue);\n",
    "\n",
    "        return(putprice)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw7MC_1 = MC(M=10000,N=4,seed=0,algorithm='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Policy optimization approach, problem 1b\n",
    "#\n",
    "# If b<<0 then this function essentially returns nearly 1 if X<a, or nearly 0 if X>a\n",
    "# but with some smoothing of the discontinuity, using a sigmoid function, to help the optimizer\n",
    "\n",
    "def softExercise(X,a,b):\n",
    "    return 1/(1+np.exp(np.minimum(-b*(X-a), 709)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Policy optimization approach, problem 1b\n",
    "\n",
    "def negofMCaverageOfExpectedPayouts(coefficients, x, exercisePayoff, continuationPayoff):\n",
    "\n",
    "    p = softExercise(x,*coefficients)    \n",
    "\n",
    "    # p and exercisePayoff and continuationPayoff are all length-M arrays\n",
    "\n",
    "    return -np.sum(p*exercisePayoff + (1-p)*continuationPayoff)\n",
    "# FILL THIS IN\n",
    "\n",
    "## You fill in, what to return.  It should be the negative of the expression inside the max() on the homework sheet.\n",
    "## Need to take the negative because we are calling \"minimize\" but we want to do _maximization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10039301505422576"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw7MC_1.price_americanPut_GBM(hw7contract,hw7dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1000405946128757"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw7MC_2 = MC(M=10000,N=4,seed=0,algorithm='policy')\n",
    "hw7MC_2.price_americanPut_GBM(hw7contract,hw7dynamics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\frac{\\partial^2 F}{\\partial u^2} = \\mathbb{E} \\big[exp(iuR_1+ivR_2)(i R_1)^2\\big]\\\\\n",
    "\\frac{\\partial^2 F}{\\partial u^2}(0,0) = \\mathbb{E} \\big[(i R_1)^2\\big] = \\mathbb{E}\\big[-R_1^2\\big]\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly,\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial^2 F}{\\partial v^2} = \\mathbb{E} \\big[exp(iuR_1+ivR_2)(i R_2)^2\\big]\\\\\n",
    "\\frac{\\partial^2 F}{\\partial v^2}(0,0) = \\mathbb{E} \\big[(i R_2)^2\\big] = \\mathbb{E}\\big[-R_2^2\\big]\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E} (R_1^2 + R_2^2) &= \\mathbb{E} (R_1^2) + \\mathbb{E}(R_2^2) \\\\\n",
    "    &= -\\frac{\\partial^2 F}{\\partial u^2}(0,0) -\\frac{\\partial^2 F}{\\partial v^2}(0,0)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\phi(w) &= \\mathbb{E} \\big[exp(iw(4R_1-3R_2))\\big] \\\\\n",
    "    &= \\mathbb{E} \\big[exp(i4wR_1-i3wR_2)\\big] \\\\\n",
    "    &= F(4w,-3w)\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "G(x,y) &= \\mathbb{E} \\big[exp(ixR_3+iyR_4))\\big] \\\\\n",
    "    &= \\mathbb{E} \\big[exp(ixR_3)\\big]\\mathbb{E} \\big[exp(iyR_4)\\big]  \\text{(Since R_3 and R_4 are independent)} \\\\ \n",
    "    &= F(x,0)F(0,y)\\\\\n",
    "\\end{aligned}$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
